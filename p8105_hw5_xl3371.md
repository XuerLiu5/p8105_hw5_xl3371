p8105_hw5_xl3371
================
Xuer Liu
2023-11-15

## Problem 2

``` r
# read multiple CSV files from data folder
all_file_df <- 
  tibble(files = list.files("./data/"),
         path = str_c("./data/", files)) %>%
  mutate(data = map(path, read_csv)) %>%
  unnest()
```

``` r
# create a tidy dataframe
tidy_df <- all_file_df %>%
  mutate(files = sub("\\.csv$", "", files), 
         arm = substr(files, 1, 3)) %>%
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "observation",
               names_prefix = "week_") %>%
  select(arm, subject_id = files, week, observation) %>%
  mutate(week = as.numeric(week))
```

``` r
# make a spaghetti plot showing observations on each subject over time
tidy_df %>%
  ggplot(aes(x = week, y = observation, group = subject_id, color = arm)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~arm) +
  labs(title = "Spaghetti plot",
       x = "Week",
       y = "Observation",
       color = "Treatment Arm")
```

<img src="p8105_hw5_xl3371_files/figure-gfm/spaghetti plot-1.png" width="90%" />

The spread of observations in the experimental group (blue lines) seems
to be wider than in the control group, which might indicate greater
variability in response to the experimental condition.

## Problem 3

``` r
set.seed(371)

sim_t_test <- function(mu, n = 30, sd = 5) {
  sim_data <- rnorm(n, mean = mu, sd = sd)
  t_test_result <- t.test(sim_data) %>% 
    broom::tidy() %>% 
    select(estimate, p.value)
  return(t_test_result)
}

run_simulations <- function(mu_vector, n_iterations) {
  results <- map_df(mu_vector, ~{
    tibble(mu = .x, t_test_df = replicate(n_iterations, sim_t_test(.x), simplify = FALSE))
  }) %>% 
    unnest(cols = c(t_test_df))

  return(results)
}

# Run simulations for mu = 0
mean_0_results <- run_simulations(mu_vector = 0, n_iterations = 5000)

# Run simulations for mu = 1:6
mu_repeat_results <- run_simulations(mu_vector = 1:6, n_iterations = 5000)
```

``` r
power_data <- mu_repeat_results %>%
  group_by(mu) %>%
  summarize(rejection_proportion = mean(p.value < 0.05, na.rm = TRUE)) %>%
  ungroup()

ggplot(power_data, aes(x = mu, y = rejection_proportion)) +
  geom_line() +
  geom_point() +
  labs(x = "True Mean (μ)", 
       y = "Power of the Test (Proportion of Rejections)", 
       title = "Power Curve") +
  theme_minimal()
```

<img src="p8105_hw5_xl3371_files/figure-gfm/plot of power-1.png" width="90%" />

As the true mean $\mu$ increases, the power of the test also increases.
This indicates a positive association between effect size and power.

``` r
# Combine the results for mu = 0 and mu = 1:6
all_results <- bind_rows(mean_0_results, mu_repeat_results)

average_estimates <- all_results %>%
  group_by(mu) %>%
  summarize(average_estimate = mean(estimate), .groups = 'drop')

average_estimates_rejected <- all_results %>%
  group_by(mu) %>%
  filter(p.value < 0.05) %>%
  summarize(average_estimate_rejected = mean(estimate), .groups = 'drop')
```

``` r
ggplot(average_estimates, aes(x = mu, y = average_estimate)) +
  geom_point() +
  geom_line() +
  labs(title = "Average Estimate of μ̂ vs. True Value of μ",
       x = "True Value of μ", y = "Average Estimate of μ̂")
```

<img src="p8105_hw5_xl3371_files/figure-gfm/plot of average estimate-1.png" width="90%" />

``` r
ggplot(average_estimates, aes(x = mu, y = average_estimate)) +
  geom_point() +
  geom_line() +
  geom_point(data = average_estimates_rejected, aes(x = mu, y = average_estimate_rejected), color = "red") +
  geom_line(data = average_estimates_rejected, aes(x = mu, y = average_estimate_rejected), color = "red") +
  labs(title = "Average Estimate of μ̂ vs. True Value of μ (Overlay for Rejected Null)",
       x = "True Value of μ", y = "Average Estimate of μ̂")
```

<img src="p8105_hw5_xl3371_files/figure-gfm/overlay the average estimates for rejected null hypothesis-1.png" width="90%" />

The sample average $\hat\mu$ across tests for which the null is rejected
is not approximately equal to the true value of $\mu$, especially at
lower true values, since the effect size is relatively small and the
power is relatively low.

However, the sample average $\hat\mu$ across tests for which the null
was rejected is getting closer and closer to the true value as the true
mean getting larger due to the positive association between effect size
and power.
